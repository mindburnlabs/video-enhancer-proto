"""
VSRM (Video Super-Resolution Mamba) Handler
Primary VSR with clip-wise linear-time temporal blocks and spatialâ†”temporal Mamba modules.
"""

"""
MIT License

Copyright (c) 2024 Video Enhancement Project

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""


import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np
from pathlib import Path
import logging
from typing import Optional, Dict, Tuple, List
import tempfile
import json

from models.backbones.mamba import EAMambaVideoBlock, SpatialTemporalMamba
from utils.video_utils import VideoUtils
from utils.performance_monitor import track_enhancement_performance, get_performance_tracker

logger = logging.getLogger(__name__)

class VSRMNetwork(nn.Module):
    """VSRM network with Mamba-based temporal processing."""
    
    def __init__(self, 
                 num_frames=7,
                 scale=4, 
                 num_channels=3,
                 embed_dim=64,
                 num_layers=6,
                 d_state=16):
        super().__init__()
        self.num_frames = num_frames
        self.scale = scale
        self.embed_dim = embed_dim
        
        # Input embedding
        self.input_conv = nn.Conv3d(num_channels, embed_dim, 
                                   kernel_size=(1, 3, 3), 
                                   padding=(0, 1, 1))
        
        # Mamba backbone layers
        self.mamba_layers = nn.ModuleList([
            EAMambaVideoBlock(
                dim=embed_dim,
                num_frames=num_frames,
                d_state=d_state,
                num_layers=2
            ) for _ in range(num_layers)
        ])
        
        # Deformable cross-Mamba alignment
        self.cross_alignment = DeformableCrossMamba(embed_dim, num_frames)
        
        # Reconstruction head
        self.reconstruction = nn.Sequential(
            nn.Conv3d(embed_dim, embed_dim * 4, kernel_size=(1, 3, 3), padding=(0, 1, 1)),
            nn.GELU(),
            nn.Conv3d(embed_dim * 4, num_channels * (scale ** 2), 
                     kernel_size=(1, 3, 3), padding=(0, 1, 1))
        )
        
        # Pixel shuffle for upsampling
        self.pixel_shuffle = nn.PixelShuffle(scale)
        
    def forward(self, x):
        """Forward pass for VSRM.
        
        Args:
            x: Input tensor of shape (B, C, T, H, W)
            
        Returns:
            Enhanced tensor of shape (B, C, T, H*scale, W*scale)
        """
        B, C, T, H, W = x.shape
        
        # Input embedding
        x = self.input_conv(x)  # (B, embed_dim, T, H, W)
        
        # Permute to Mamba format (B, T, H, W, C)
        x = x.permute(0, 2, 3, 4, 1)
        
        # Process through Mamba layers
        for layer in self.mamba_layers:
            residual = x
            x = layer(x)
            x = residual + x  # Residual connection
        
        # Cross-frame alignment
        x = self.cross_alignment(x)
        
        # Back to conv format (B, C, T, H, W)
        x = x.permute(0, 4, 1, 2, 3)
        
        # Reconstruction
        x = self.reconstruction(x)  # (B, C*(scale^2), T, H, W)
        
        # Pixel shuffle for each frame
        output_frames = []
        for t in range(T):
            frame = x[:, :, t, :, :]  # (B, C*(scale^2), H, W)
            upsampled = self.pixel_shuffle(frame)  # (B, C, H*scale, W*scale)
            output_frames.append(upsampled)
        
        output = torch.stack(output_frames, dim=2)  # (B, C, T, H*scale, W*scale)
        
        return output

class DeformableCrossMamba(nn.Module):
    """Deformable cross-Mamba alignment for temporal consistency."""
    
    def __init__(self, dim, num_frames):
        super().__init__()
        self.dim = dim
        self.num_frames = num_frames
        
        # Offset prediction for deformable alignment
        self.offset_conv = nn.Sequential(
            nn.Conv3d(dim, dim, kernel_size=(3, 3, 3), padding=(1, 1, 1)),
            nn.GELU(),
            nn.Conv3d(dim, 2, kernel_size=(3, 3, 3), padding=(1, 1, 1))  # x,y offsets
        )
        
        # Cross-frame Mamba
        self.cross_mamba = SpatialTemporalMamba(dim, num_frames)
        
    def forward(self, x):
        """Apply deformable cross-frame alignment.
        
        Args:
            x: Input tensor of shape (B, T, H, W, C)
            
        Returns:
            Aligned tensor of same shape
        """
        B, T, H, W, C = x.shape
        
        # Convert to conv format for offset prediction
        x_conv = x.permute(0, 4, 1, 2, 3)  # (B, C, T, H, W)
        
        # Predict alignment offsets
        offsets = self.offset_conv(x_conv)  # (B, 2, T, H, W)
        
        # Apply deformable alignment (simplified)
        aligned_x = self._apply_deformable_alignment(x_conv, offsets)
        
        # Back to Mamba format
        aligned_x = aligned_x.permute(0, 2, 3, 4, 1)  # (B, T, H, W, C)
        
        # Apply cross-frame Mamba
        output = self.cross_mamba(aligned_x)
        
        return output
    
    def _apply_deformable_alignment(self, x, offsets):
        """Apply deformable alignment using grid_sample with predicted offsets.
        x: (B, C, T, H, W), offsets: (B, 2, T, H, W) with dx, dy in pixels
        """
        B, C, T, H, W = x.shape
        device = x.device
        dtype = x.dtype
        # Normalize offsets to [-1,1]
        off = offsets.to(device=device, dtype=dtype)
        norm_off_x = off[:, 0] / max(W - 1, 1)
        norm_off_y = off[:, 1] / max(H - 1, 1)
        ys, xs = torch.meshgrid(
            torch.linspace(-1, 1, H, device=device, dtype=dtype),
            torch.linspace(-1, 1, W, device=device, dtype=dtype), indexing='ij')
        base_grid = torch.stack((xs, ys), dim=-1)  # (H, W, 2)
        aligned_frames = []
        for t in range(T):
            grid = base_grid.unsqueeze(0).repeat(B, 1, 1, 1)
            grid_x = grid[..., 0] + 2.0 * norm_off_x[:, t]
            grid_y = grid[..., 1] + 2.0 * norm_off_y[:, t]
            grid_t = torch.stack((grid_x, grid_y), dim=-1)
            frame = x[:, :, t]
            aligned = F.grid_sample(frame, grid_t, mode='bilinear', padding_mode='border', align_corners=True)
            aligned_frames.append(aligned.unsqueeze(2))
        return torch.cat(aligned_frames, dim=2)

class VSRMHandler:
    """VSRM Video Super-Resolution Handler with Mamba backbone."""
    
    def __init__(self, 
                 model_path: Optional[str] = None,
                 device: str = "cuda",
                 scale: int = 4,
                 num_frames: int = 7,
                 tile_size: int = 512,
                 tile_overlap: int = 32):
        
        self.device = torch.device(device)
        self.scale = scale
        self.num_frames = num_frames
        self.tile_size = tile_size
        self.tile_overlap = tile_overlap
        
        logger.info("ðŸ”¥ Initializing VSRM Handler...")
        logger.info(f"   Device: {device}")
        logger.info(f"   Scale: {scale}x")
        logger.info(f"   Frames: {num_frames}")
        
        # Initialize network
        self.model = VSRMNetwork(
            num_frames=num_frames,
            scale=scale,
            embed_dim=64,
            num_layers=6
        ).to(self.device)
        
        # Resolve and load weights
        resolved_model_path = self._resolve_model_path(model_path)
        if resolved_model_path and Path(resolved_model_path).exists():
            self._load_model(resolved_model_path)
        else:
            logger.warning("No VSRM model weights found, using random initialization")
        
        self.model.eval()
        self.video_utils = VideoUtils()
        
        logger.info("âœ… VSRM Handler initialized")
    
    def _resolve_model_path(self, model_path: Optional[str]) -> Optional[str]:
        if model_path:
            return str(model_path)
        import os, json
        # ENV: VSRM_DIR
        d = os.getenv('VSRM_DIR')
        if d and Path(d).exists():
            candidate = self._find_weight_file_in_dir(d)
            if candidate:
                logger.info(f"ðŸ”Ž Resolved VSRM weights via VSRM_DIR: {candidate}")
                return candidate
        # Registry
        registry_path = Path(__file__).resolve().parents[3] / "config" / "model_registry.json"
        if registry_path.exists():
            try:
                data = json.loads(registry_path.read_text())
                for m in data.get("models", []):
                    if m.get("id") in ["vsrm"] and m.get("enabled", False):
                        local_path = m.get("local_path")
                        if local_path and Path(local_path).exists():
                            candidate = self._find_weight_file_in_dir(local_path)
                            if candidate:
                                logger.info(f"ðŸ”Ž Resolved VSRM weights via registry: {candidate}")
                                return candidate
            except Exception as e:
                logger.warning(f"Could not parse model_registry.json: {e}")
        return None

    def _find_weight_file_in_dir(self, d: str) -> Optional[str]:
        p = Path(d)
        patterns = ["*.safetensors", "*.pt", "*.pth"]
        for pat in patterns:
            matches = list(p.rglob(pat))
            matches_sorted = sorted(matches, key=lambda x: (x.suffix != ".safetensors", len(str(x))))
            if matches_sorted:
                return str(matches_sorted[0])
        return None

    def _load_model(self, model_path: str):
        """Load pretrained VSRM weights."""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # Handle different checkpoint formats
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
            
            self.model.load_state_dict(state_dict, strict=False)
            logger.info(f"âœ… Loaded VSRM weights from {model_path}")
            
        except Exception as e:
            logger.error(f"Failed to load model weights: {e}")
            logger.info("Using random initialization")
    
    @track_enhancement_performance('vsrm')
    def enhance_video(self, 
                     input_path: str, 
                     output_path: str,
                     window: int = None,
                     stride: int = None,
                     fp16: bool = True) -> Dict:
        """Enhance video using VSRM.
        
        Args:
            input_path: Path to input video
            output_path: Path to output video
            window: Processing window size (default: num_frames)
            stride: Stride between windows (default: num_frames//2)
            fp16: Use half precision for efficiency
            
        Returns:
            Processing statistics
        """
        window = window or self.num_frames
        stride = stride or max(1, self.num_frames // 2)
        
        logger.info(f"ðŸŽ¬ Processing video with VSRM...")
        logger.info(f"   Input: {input_path}")
        logger.info(f"   Output: {output_path}")
        logger.info(f"   Window: {window}, Stride: {stride}")
        
        try:
            # Get video metadata
            metadata = self.video_utils.get_video_metadata(input_path)
            fps = metadata['fps']
            total_frames = metadata['frame_count']
            
            # Setup video capture and writer
            cap = cv2.VideoCapture(input_path)
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out_height = metadata['height'] * self.scale
            out_width = metadata['width'] * self.scale
            
            out = cv2.VideoWriter(output_path, fourcc, fps, (out_width, out_height))
            
            # Process in sliding windows
            frame_buffer = []
            processed_count = 0
            
            with torch.cuda.amp.autocast(enabled=fp16):
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    frame_buffer.append(frame)
                    
                    # Process when buffer is full
                    if len(frame_buffer) >= window:
                        enhanced_frames = self._process_frame_window(
                            frame_buffer[:window], fp16
                        )
                        
                        # Write enhanced frames (stride determines how many)
                        write_count = min(stride, len(enhanced_frames))
                        for i in range(write_count):
                            if enhanced_frames[i] is not None:
                                out.write(enhanced_frames[i])
                                processed_count += 1
                        
                        # Slide the window
                        frame_buffer = frame_buffer[stride:]
                
                # Process remaining frames
                if len(frame_buffer) > 0:
                    enhanced_frames = self._process_frame_window(frame_buffer, fp16)
                    for frame in enhanced_frames:
                        if frame is not None:
                            out.write(frame)
                            processed_count += 1
            
            cap.release()
            out.release()
            
            stats = {
                'input_frames': total_frames,
                'output_frames': processed_count,
                'scale_factor': self.scale,
                'processing_mode': 'vsrm_mamba',
                'fp16': fp16,
                # Performance tracking fields
                'frames_processed': processed_count,
                'input_resolution': (metadata['height'], metadata['width']),
                'output_resolution': (out_height, out_width),
                'quality_score': self._estimate_quality_score(processed_count)
            }
            
            logger.info(f"âœ… VSRM processing completed")
            logger.info(f"   Processed: {processed_count} frames")
            
            return stats
            
        except Exception as e:
            logger.error(f"VSRM processing failed: {e}")
            raise
    
    def _process_frame_window(self, frames: List[np.ndarray], fp16: bool = True) -> List[np.ndarray]:
        """Process a window of frames with VSRM."""
        try:
            # Convert frames to tensor
            input_tensor = []
            for frame in frames:
                # Convert BGR to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # Normalize to [0, 1]
                frame_norm = frame_rgb.astype(np.float32) / 255.0
                # Convert to tensor (C, H, W)
                frame_tensor = torch.from_numpy(frame_norm.transpose(2, 0, 1))
                input_tensor.append(frame_tensor)
            
            # Stack to (C, T, H, W) and add batch dimension
            input_tensor = torch.stack(input_tensor, dim=1).unsqueeze(0).to(self.device)
            
            # Tile-based processing for large frames
            if input_tensor.shape[-1] > self.tile_size or input_tensor.shape[-2] > self.tile_size:
                output_tensor = self._tile_process(input_tensor, fp16)
            else:
                with torch.no_grad():
                    if fp16:
                        input_tensor = input_tensor.half()
                    output_tensor = self.model(input_tensor)
            
            # Convert back to numpy frames
            enhanced_frames = []
            for t in range(output_tensor.shape[2]):
                frame_tensor = output_tensor[0, :, t, :, :].cpu().float()
                # Clamp and denormalize
                frame_tensor = torch.clamp(frame_tensor, 0, 1)
                frame_np = (frame_tensor.numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                # Convert back to BGR
                frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)
                enhanced_frames.append(frame_bgr)
            
            return enhanced_frames
            
        except Exception as e:
            logger.error(f"Frame window processing failed: {e}")
            # Return original frames as fallback
            return frames
    
    def _tile_process(self, input_tensor: torch.Tensor, fp16: bool = True) -> torch.Tensor:
        """Process large frames using tiling strategy."""
        B, C, T, H, W = input_tensor.shape
        tile_h = tile_w = self.tile_size
        overlap = self.tile_overlap
        
        # Calculate tiles
        tiles_h = (H + tile_h - overlap - 1) // (tile_h - overlap)
        tiles_w = (W + tile_w - overlap - 1) // (tile_w - overlap)
        
        output_tensor = torch.zeros(B, C, T, H * self.scale, W * self.scale, 
                                   device=self.device, dtype=input_tensor.dtype)
        
        for i in range(tiles_h):
            for j in range(tiles_w):
                # Calculate tile boundaries
                start_h = i * (tile_h - overlap)
                end_h = min(start_h + tile_h, H)
                start_w = j * (tile_w - overlap)
                end_w = min(start_w + tile_w, W)
                
                # Extract tile
                tile_input = input_tensor[:, :, :, start_h:end_h, start_w:end_w]
                
                # Process tile
                with torch.no_grad():
                    if fp16:
                        tile_input = tile_input.half()
                    tile_output = self.model(tile_input)
                
                # Place tile in output (handle overlaps by averaging)
                out_start_h = start_h * self.scale
                out_end_h = end_h * self.scale  
                out_start_w = start_w * self.scale
                out_end_w = end_w * self.scale
                
                output_tensor[:, :, :, out_start_h:out_end_h, out_start_w:out_end_w] = tile_output
        
        return output_tensor
    
    def _estimate_quality_score(self, frames_processed: int) -> float:
        """Estimate quality score based on processing statistics."""
        # Simple quality estimation based on frame count and model properties
        base_score = 0.8  # Base quality for VSRM
        
        # Adjust based on processing completeness
        if frames_processed > 0:
            # Higher scores for more frames processed successfully
            frame_bonus = min(0.15, frames_processed / 1000.0 * 0.15)
            base_score += frame_bonus
        
        return min(base_score, 1.0)
    
    def get_model_info(self) -> Dict:
        """Get information about the VSRM model."""
        return {
            'name': 'VSRM',
            'description': 'Video Super-Resolution with Mamba backbone',
            'scale': self.scale,
            'num_frames': self.num_frames,
            'parameters': sum(p.numel() for p in self.model.parameters()),
            'device': str(self.device),
            'architecture': 'Mamba-based VSR with deformable alignment'
        }
